{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initiate the required libraries for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DQN\n",
    "from gym.wrappers import monitoring\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the enviroment in which the agent will be trained. Note that the agent will receive a positive reward only in case of win (+1), in case of draw the reward will be 0 and in case of lose the reward will be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cmp(a, b):\n",
    "    if a > b:\n",
    "        return 1\n",
    "    elif a < b:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "class SimpleBlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleBlackjackEnv, self).__init__()\n",
    "        self.deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10] * 4  # a full deck\n",
    "        random.shuffle(self.deck)  # shuffle the deck\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(low=0, high=11, shape=(23,), dtype=int)\n",
    "        \n",
    "    def draw_card(self):\n",
    "        return self.deck.pop()\n",
    "        \n",
    "    def draw_hand(self):\n",
    "        return [self.draw_card(), self.draw_card()]\n",
    "\n",
    "    def usable_ace(self, hand):\n",
    "        return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "    def sum_hand(self, hand):\n",
    "        if self.usable_ace(hand):\n",
    "            return sum(hand) + 10\n",
    "        return sum(hand)\n",
    "\n",
    "    def is_bust(self, hand):\n",
    "        return self.sum_hand(hand) > 21\n",
    "\n",
    "    def score(self, hand):\n",
    "        return 0 if self.is_bust(hand) else self.sum_hand(hand)\n",
    "\n",
    "    def reset(self):\n",
    "        if len(self.deck) < 15:\n",
    "            self.deck = [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
    "                         10, 10, 10, 10] * 4\n",
    "            random.shuffle(self.deck)\n",
    "        self.dealer = self.draw_hand()\n",
    "        self.player = self.draw_hand()\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # hit\n",
    "            self.player.append(self.draw_card())\n",
    "            if self.is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -1.0\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0.0\n",
    "        else:  # stick\n",
    "            done = True\n",
    "            while self.sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(self.draw_card())\n",
    "            reward = cmp(self.score(self.player), self.score(self.dealer))\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        player_obs = self.player + [0] * (11 - len(self.player))\n",
    "        dealer_obs = self.dealer + [0] * (11 - len(self.dealer))\n",
    "        usable_ace_obs = [1] if self.usable_ace(self.player) else [0]\n",
    "        return np.array(player_obs + dealer_obs + usable_ace_obs)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode != 'human':\n",
    "            raise NotImplementedError()\n",
    "        return f\"Player hand: {self.player}, Dealer hand: {self.dealer}\"\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Testing the environment to ensure it initializes and steps correctly\n",
    "env = SimpleBlackjackEnv()\n",
    "obs = env.reset()\n",
    "print(env.render())\n",
    "obs, reward, done, _ = env.step(1)\n",
    "print(env.render())\n",
    "obs, reward, done, _ = env.step(0)\n",
    "print(env.render())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model we will be using a stable_baselines3 algorithm, which will only need to be called and trained. Since the performance with the standard hyperparameters are poor we also will use optuna to find the best hyperparameters in order to increase the win rate of the agent. Note that the trials have been set to 5 because of compoutational and running time purposes, with an higher number of trials optuna will surely find more accurate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-29 19:56:57,977] A new study created in memory with name: no-name-1316754c-3b42-4677-b81e-71dd79f6bfa2\n",
      "/Users/tommasoagudio/anaconda3/envs/Reinforcement_Learning/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/Users/tommasoagudio/anaconda3/envs/Reinforcement_Learning/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1222`, after every 19 untruncated mini-batches, there will be a truncated mini-batch of size 6\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1222 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2023-10-29 19:57:23,588] Trial 0 finished with value: -0.436 and parameters: {'learning_rate': 0.0015176242402698446, 'gamma': 0.9442402649180488, 'n_steps': 1222, 'ent_coef': 0.0982024564895372}. Best is trial 0 with value: -0.436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0015176242402698446, 'gamma': 0.9442402649180488, 'n_steps': 1222, 'ent_coef': 0.0982024564895372}\n",
      "Using cpu device\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Discrete(2) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/tommasoagudio/Documents/GitHub/GamblingRL/MAIN.ipynb Cella 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tommasoagudio/Documents/GitHub/GamblingRL/MAIN.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m env \u001b[39m=\u001b[39m Monitor(env)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tommasoagudio/Documents/GitHub/GamblingRL/MAIN.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m env \u001b[39m=\u001b[39m DummyVecEnv([\u001b[39mlambda\u001b[39;00m: env])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tommasoagudio/Documents/GitHub/GamblingRL/MAIN.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbest_params, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, tensorboard_log\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./PPO_logs\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tommasoagudio/Documents/GitHub/GamblingRL/MAIN.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Reinforcement_Learning/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:104\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m ):\n\u001b[0;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    105\u001b[0m         policy,\n\u001b[1;32m    106\u001b[0m         env,\n\u001b[1;32m    107\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    108\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m    109\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    110\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[1;32m    111\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[1;32m    112\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[1;32m    113\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[1;32m    114\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m    115\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m    116\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m    117\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m    118\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m    119\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    120\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    122\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    123\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    124\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[1;32m    125\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[1;32m    126\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[1;32m    127\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[1;32m    128\u001b[0m         ),\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     \u001b[39m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/anaconda3/envs/Reinforcement_Learning/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:81\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     60\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     82\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m     83\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m     84\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     85\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m     86\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     87\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     88\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m     89\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m     90\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     91\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     92\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m     93\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m     94\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/anaconda3/envs/Reinforcement_Learning/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:180\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vec_normalize_env \u001b[39m=\u001b[39m unwrap_vec_normalize(env)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m supported_action_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[1;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe algorithm only supports \u001b[39m\u001b[39m{\u001b[39;00msupported_action_spaces\u001b[39m}\u001b[39;00m\u001b[39m as action spaces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m was provided\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m support_multi_env \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: the model does not support multiple envs; it requires \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma single vectorized environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Discrete(2) was provided"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(model, env, num_games=1000): #\n",
    "    wins = 0                                    \n",
    "    for _ in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "        if reward == 1.0:\n",
    "            wins += 1\n",
    "    win_rate = wins / num_games\n",
    "    return win_rate\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 16, 2048, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 1e-8, 1e-1, log=True)\n",
    "    \n",
    "    env = DummyVecEnv([lambda: SimpleBlackjackEnv()])\n",
    "    model = PPO(\"MlpPolicy\", env, learning_rate=learning_rate, gamma=gamma, n_steps=n_steps, ent_coef=ent_coef, verbose=0)\n",
    "    \n",
    "    model.learn(total_timesteps=100000)\n",
    "    \n",
    "    win_rate = evaluate_agent(model, env)\n",
    "    \n",
    "    return -win_rate  \n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "print(study.best_params) # <-- best hyperparameters found (not necessarily the same as best model)\n",
    "\n",
    "\n",
    "#training the agent with the best hyperparameters found by optuna\n",
    "best_params = study.best_params\n",
    "env = DummyVecEnv([lambda: SimpleBlackjackEnv()])\n",
    "model = PPO(\"MlpPolicy\", env, **best_params, verbose=1, tensorboard_log='./PPO_logs')\n",
    "model.learn(total_timesteps=50000)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_verbose(model, num_games=10000):\n",
    "    action_mapping = {0: \"Stick\", 1: \"Hit\"}\n",
    "    won_games = 0\n",
    "    for _ in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        print(\"\\nStarting a new game...\")\n",
    "        \n",
    "        while not done:\n",
    "            player_hand_length = np.sum(obs[0][:11].astype(int) != 0)\n",
    "            dealer_hand_length = np.sum(obs[0][11:22].astype(int) != 0)\n",
    "            \n",
    "            player_hand = obs[0][:11][:player_hand_length]\n",
    "            dealer_hand = obs[0][11:22][:dealer_hand_length]\n",
    "            \n",
    "            print(f\"Player's hand: {player_hand}\")\n",
    "            print(f\"Dealer's visible card: {dealer_hand[0]}\")\n",
    "            \n",
    "            action, _ = model.predict(obs)\n",
    "            print(f\"Agent's action: {action_mapping[int(action)]}\")  \n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        print(f\"Player's final hand: {player_hand}\")\n",
    "        print(f\"Dealer's final hand: {dealer_hand}\")\n",
    "        \n",
    "        if reward > 0:\n",
    "            print(\"Result: Won!\")\n",
    "            won_games += 1\n",
    "        elif reward < 0:\n",
    "            print(\"Result: Lost!\")\n",
    "        else:\n",
    "            print(\"Result: Draw!\")\n",
    "        print('-'*40)\n",
    "    print(f\"Agent won {won_games} out of {num_games} games.\")\n",
    "    return won_games  # Return the number of won games\n",
    "\n",
    "def simulate_games(model, num_simulations=10, games_per_simulation=[100, 500, 1000, 5000, 10000]):\n",
    "    win_rates = []\n",
    "    for num_games in games_per_simulation:\n",
    "        total_wins = 0\n",
    "        for _ in range(num_simulations):\n",
    "            won_games = test_agent_verbose(model, num_games)  # Assumes test_agent returns the number of won games\n",
    "            if won_games is not None:  # Check if won_games is not None before adding to total_wins\n",
    "                total_wins += won_games\n",
    "        average_win_rate = total_wins / (num_games * num_simulations)\n",
    "        win_rates.append(average_win_rate)\n",
    "        print(f\"Average winning rate for {num_games} games: {average_win_rate * 100}%\")\n",
    "    return win_rates\n",
    "simulate_games(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the agen won at most 45% of the games, which is way less then the expected 49% which is the world's average winning rate. But for this enviroment we must keep in mind that we do not have the possibility to split, even if it would not change things too much, because if we split we will be simply playing 2 hands at the same time, meaning that it will not change the winning rate in most cases.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAN'T RUN THE CODE BELOW DUE TO HIGH COMPUTATIONAL POWER REQUIRED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from gym.wrappers import monitoring\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create environment\n",
    "env = DummyVecEnv([lambda: SimpleBlackjackEnv()])\n",
    "\n",
    "# Initialize agent\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, buffer_size= 50000)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"dqn_blackjack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_dqn(model, num_games=10000):\n",
    "    action_mapping = {0: \"Stick\", 1: \"Hit\"}\n",
    "    won_games = 0\n",
    "    for _ in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        print(\"\\nStarting a new game...\")\n",
    "        \n",
    "        while not done:\n",
    "            player_hand_length = np.sum(obs[0][:11].astype(int) != 0)\n",
    "            dealer_hand_length = np.sum(obs[0][11:22].astype(int) != 0)\n",
    "            \n",
    "            player_hand = obs[0][:11][:player_hand_length]\n",
    "            dealer_hand = obs[0][11:22][:dealer_hand_length]\n",
    "            \n",
    "            print(f\"Player's hand: {player_hand}\")\n",
    "            print(f\"Dealer's visible card: {dealer_hand[0]}\")\n",
    "            \n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            print(f\"Agent's action: {action_mapping[int(action)]}\")  \n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        print(f\"Player's final hand: {player_hand}\")\n",
    "        print(f\"Dealer's final hand: {dealer_hand}\")\n",
    "        \n",
    "        if reward > 0:\n",
    "            print(\"Result: Won!\")\n",
    "            won_games += 1\n",
    "        elif reward < 0:\n",
    "            print(\"Result: Lost!\")\n",
    "        else:\n",
    "            print(\"Result: Draw!\")\n",
    "        print('-'*40)\n",
    "    print(f\"Agent won {won_games} out of {num_games} games.\")\n",
    "    return won_games  # Return the number of won games\n",
    "\n",
    "# Test the DQN model\n",
    "test_agent_dqn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from gym.wrappers import monitoring\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "env = DummyVecEnv([lambda: SimpleBlackjackEnv()])\n",
    "\n",
    "model = DQN.load(\"./RisultatiAlessio/dqn_blackjack\")\n",
    "def test_agent_dqn(model, num_games=10000):\n",
    "    action_mapping = {0: \"Stick\", 1: \"Hit\"}\n",
    "    won_games = 0\n",
    "    for _ in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        print(\"\\nStarting a new game...\")\n",
    "        \n",
    "        while not done:\n",
    "            player_hand_length = np.sum(obs[0][:11].astype(int) != 0)\n",
    "            dealer_hand_length = np.sum(obs[0][11:22].astype(int) != 0)\n",
    "            \n",
    "            player_hand = obs[0][:11][:player_hand_length]\n",
    "            dealer_hand = obs[0][11:22][:dealer_hand_length]\n",
    "            \n",
    "            print(f\"Player's hand: {player_hand}\")\n",
    "            print(f\"Dealer's visible card: {dealer_hand[0]}\")\n",
    "            \n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            print(f\"Agent's action: {action_mapping[int(action)]}\")  \n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        print(f\"Player's final hand: {player_hand}\")\n",
    "        print(f\"Dealer's final hand: {dealer_hand}\")\n",
    "        \n",
    "        if reward > 0:\n",
    "            print(\"Result: Won!\")\n",
    "            won_games += 1\n",
    "        elif reward < 0:\n",
    "            print(\"Result: Lost!\")\n",
    "        else:\n",
    "            print(\"Result: Draw!\")\n",
    "        print('-'*40)\n",
    "    print(f\"Agent won {won_games} out of {num_games} games.\")\n",
    "    return won_games  # Return the number of won games\n",
    "\n",
    "# Test the DQN model\n",
    "test_agent_dqn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"./RisultatiAlessio/ppo_blackjack\")\n",
    "def test_agent_verbose(model, num_games=10000):\n",
    "    action_mapping = {0: \"Stick\", 1: \"Hit\"}\n",
    "    won_games = 0\n",
    "    for _ in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        print(\"\\nStarting a new game...\")\n",
    "        \n",
    "        while not done:\n",
    "            player_hand_length = np.sum(obs[0][:11].astype(int) != 0)\n",
    "            dealer_hand_length = np.sum(obs[0][11:22].astype(int) != 0)\n",
    "            \n",
    "            player_hand = obs[0][:11][:player_hand_length]\n",
    "            dealer_hand = obs[0][11:22][:dealer_hand_length]\n",
    "            \n",
    "            print(f\"Player's hand: {player_hand}\")\n",
    "            print(f\"Dealer's visible card: {dealer_hand[0]}\")\n",
    "            \n",
    "            action, _ = model.predict(obs)\n",
    "            print(f\"Agent's action: {action_mapping[int(action)]}\")  \n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        print(f\"Player's final hand: {player_hand}\")\n",
    "        print(f\"Dealer's final hand: {dealer_hand}\")\n",
    "        \n",
    "        if reward > 0:\n",
    "            print(\"Result: Won!\")\n",
    "            won_games += 1\n",
    "        elif reward < 0:\n",
    "            print(\"Result: Lost!\")\n",
    "        else:\n",
    "            print(\"Result: Draw!\")\n",
    "        print('-'*40)\n",
    "    print(f\"Agent won {won_games} out of {num_games} games.\")\n",
    "    return won_games  # Return the number of won games\n",
    "\n",
    "def simulate_games(model, num_simulations=10, games_per_simulation=[100, 500, 1000, 5000, 10000]):\n",
    "    win_rates = []\n",
    "    for num_games in games_per_simulation:\n",
    "        total_wins = 0\n",
    "        for _ in range(num_simulations):\n",
    "            won_games = test_agent_verbose(model, num_games)  # Assumes test_agent returns the number of won games\n",
    "            if won_games is not None:  # Check if won_games is not None before adding to total_wins\n",
    "                total_wins += won_games\n",
    "        average_win_rate = total_wins / (num_games * num_simulations)\n",
    "        win_rates.append(average_win_rate)\n",
    "        print(f\"Average winning rate for {num_games} games: {average_win_rate * 100}%\")\n",
    "    return win_rates\n",
    "simulate_games(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
