{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initiate the required libraries for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DQN\n",
    "from gym.wrappers import monitoring\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the enviroment in which the agent will be trained. Note that the agent will receive a positive reward only in case of win (+1), in case of draw the reward will be 0 and in case of lose the reward will be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cmp(a, b):\n",
    "    if a > b:\n",
    "        return 1\n",
    "    elif a < b:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "class SimpleBlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reward_range = (-np.inf, np.inf)\n",
    "        super(SimpleBlackjackEnv, self).__init__()\n",
    "        self.deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10] * 4  # a full deck\n",
    "        random.shuffle(self.deck)  # shuffle the deck\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(low=0, high=11, shape=(23,), dtype=int)\n",
    "        \n",
    "    def draw_card(self):\n",
    "        return self.deck.pop()\n",
    "        \n",
    "    def draw_hand(self):\n",
    "        return [self.draw_card(), self.draw_card()]\n",
    "\n",
    "    def usable_ace(self, hand):\n",
    "        return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "    def sum_hand(self, hand):\n",
    "        if self.usable_ace(hand):\n",
    "            return sum(hand) + 10\n",
    "        return sum(hand)\n",
    "\n",
    "    def is_bust(self, hand):\n",
    "        return self.sum_hand(hand) > 21\n",
    "\n",
    "    def score(self, hand):\n",
    "        return 0 if self.is_bust(hand) else self.sum_hand(hand)\n",
    "\n",
    "    def reset(self):\n",
    "        if len(self.deck) < 15:\n",
    "            self.deck = [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
    "                         10, 10, 10, 10] * 4\n",
    "            random.shuffle(self.deck)\n",
    "        self.dealer = self.draw_hand()\n",
    "        self.player = self.draw_hand()\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # hit\n",
    "            self.player.append(self.draw_card())\n",
    "            if self.is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -1.0\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0.0\n",
    "        else:  # stick\n",
    "            done = True\n",
    "            while self.sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(self.draw_card())\n",
    "            reward = cmp(self.score(self.player), self.score(self.dealer))\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        player_obs = self.player + [0] * (11 - len(self.player))\n",
    "        dealer_obs = self.dealer + [0] * (11 - len(self.dealer))\n",
    "        usable_ace_obs = [1] if self.usable_ace(self.player) else [0]\n",
    "        return np.array(player_obs + dealer_obs + usable_ace_obs)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode != 'human':\n",
    "            raise NotImplementedError()\n",
    "        return f\"Player hand: {self.player}, Dealer hand: {self.dealer}\"\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "# Testing the environment to ensure it initializes and steps correctly\n",
    "env = SimpleBlackjackEnv()\n",
    "obs = env.reset()\n",
    "print(env.render())\n",
    "obs, reward, done, _ = env.step(1)\n",
    "print(env.render())\n",
    "obs, reward, done, _ = env.step(0)\n",
    "print(env.render())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will train 3 different PPO models, with the same hyperparameters but different total_timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(model, env, num_games=1000):\n",
    "    wins = 0\n",
    "    win_rates = []\n",
    "    num_games_list = []  # List to store the number of games after each logging interval\n",
    "\n",
    "    for i in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            if done and reward == 1:\n",
    "                wins += 1\n",
    "        if (i + 1) % 100 == 0:  # Log win rate every 100 games\n",
    "            win_rates.append(wins / (i + 1))\n",
    "            num_games_list.append(i + 1)  # Append the number of games played so far\n",
    "\n",
    "    # Create a DataFrame with both win rates and number of games\n",
    "    win_rate_df = pd.DataFrame({'WinRate': win_rates, 'NumGames': num_games_list})\n",
    "    win_rate_df.to_csv('PPO500k_win_rate_over_time.csv', index=False)\n",
    "    \n",
    "    return wins / num_games\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = DummyVecEnv([lambda: SimpleBlackjackEnv()])\n",
    "\n",
    "# Set hyperparameters\n",
    "params = {\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'n_steps': 256,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 10,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_range': 0.2,\n",
    "    'ent_coef': 1e-4\n",
    "}\n",
    "\n",
    "# Instantiate the model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"./blackjack_tensorboard/\", **params)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=500000)\n",
    "\n",
    "# Evaluate the model\n",
    "win_rate = evaluate_agent(model, env)\n",
    "\n",
    "def simulate_blackjack_games(env, model, num_games=10000):\n",
    "    action_frequencies = {}\n",
    "    rewards = []\n",
    "    results = []\n",
    "\n",
    "    for game in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        player_actions = []\n",
    "        player_hand_sums = []\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            player_actions.append('Hit' if action == 1 else 'Stick')\n",
    "\n",
    "            # Define state key\n",
    "            player_hand = obs[:11][obs[:11] != 0]\n",
    "            dealer_visible_card = env.dealer[0]\n",
    "            state_key = (tuple(player_hand), dealer_visible_card)\n",
    "\n",
    "            # Record action frequencies\n",
    "            if state_key not in action_frequencies:\n",
    "                action_frequencies[state_key] = {'Hit': 0, 'Stick': 0}\n",
    "            action_frequencies[state_key]['Hit' if action == 1 else 'Stick'] += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            player_hand_sums.append(env.sum_hand(player_hand))\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        player_final_hand = obs[:11][obs[:11] != 0]\n",
    "        dealer_final_hand = obs[11:22][obs[11:22] != 0]\n",
    "\n",
    "        game_results = {\n",
    "            'Game': game + 1,\n",
    "            'PlayerFinalHandSum': env.sum_hand(player_final_hand),\n",
    "            'DealerFinalHandSum': env.sum_hand(dealer_final_hand),\n",
    "            'PlayerNumCards': len(player_final_hand),\n",
    "            'DealerNumCards': len(dealer_final_hand),\n",
    "            'DealerVisibleCard': dealer_visible_card,\n",
    "            'PlayerActions': ' '.join(player_actions),\n",
    "            'PlayerHandProgression': ' '.join(map(str, player_hand_sums)),\n",
    "            'Outcome': 'Win' if reward > 0 else 'Loss' if reward < 0 else 'Draw'\n",
    "        }\n",
    "        results.append(game_results)\n",
    "\n",
    "    # Export action frequencies and rewards\n",
    "    action_freq_data = []\n",
    "    for state, actions in action_frequencies.items():\n",
    "        player_hand, dealer_card = state\n",
    "        action_freq_data.append({'PlayerHand': ' '.join(map(str, player_hand)), \n",
    "                                 'DealerVisibleCard': dealer_card,\n",
    "                                 'Hit': actions['Hit'], \n",
    "                                 'Stick': actions['Stick']})\n",
    "    \n",
    "    action_freq_df = pd.DataFrame(action_freq_data)\n",
    "    action_freq_df.to_csv('PPO500k_action_frequencies.csv', index=False)\n",
    "    \n",
    "    rewards_df = pd.DataFrame(rewards, columns=['Reward'])\n",
    "    rewards_df.to_csv('PPO500k_rewards_distribution.csv', index=False)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('PPO500k_blackjack_results.csv', index=False)\n",
    "\n",
    "    win_rate = results_df[results_df['Outcome'] == 'Win'].shape[0] / num_games\n",
    "    print(f\"\\nAgent won {results_df[results_df['Outcome'] == 'Win'].shape[0]} out of {num_games} games. Win rate: {win_rate * 100:.2f}%\")\n",
    "    return win_rate\n",
    "\n",
    "model.save(\"PPO500k\")\n",
    "env = SimpleBlackjackEnv()\n",
    "simulate_blackjack_games(env, model)\n",
    "print(f\"Win rate: {win_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the agent for 1M timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(model, env, num_games=1000):\n",
    "    wins = 0\n",
    "    win_rates = []\n",
    "    num_games_list = []  # List to store the number of games after each logging interval\n",
    "\n",
    "    for i in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            if done and reward == 1:\n",
    "                wins += 1\n",
    "        if (i + 1) % 100 == 0:  # Log win rate every 100 games\n",
    "            win_rates.append(wins / (i + 1))\n",
    "            num_games_list.append(i + 1)  # Append the number of games played so far\n",
    "\n",
    "    # Create a DataFrame with both win rates and number of games\n",
    "    win_rate_df = pd.DataFrame({'WinRate': win_rates, 'NumGames': num_games_list})\n",
    "    win_rate_df.to_csv('PPO1M_win_rate_over_time.csv', index=False)\n",
    "    \n",
    "    return wins / num_games\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = DummyVecEnv([lambda: SimpleBlackjackEnv()])\n",
    "\n",
    "# Set hyperparameters\n",
    "params = {\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'n_steps': 256,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 10,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_range': 0.2,\n",
    "    'ent_coef': 1e-4\n",
    "}\n",
    "\n",
    "# Instantiate the model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"./blackjack_tensorboard/\", **params)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100)\n",
    "\n",
    "# Evaluate the model\n",
    "win_rate = evaluate_agent(model, env)\n",
    "\n",
    "def simulate_blackjack_games(env, model, num_games=10000):\n",
    "    action_frequencies = {}\n",
    "    rewards = []\n",
    "    results = []\n",
    "\n",
    "    for game in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        player_actions = []\n",
    "        player_hand_sums = []\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            player_actions.append('Hit' if action == 1 else 'Stick')\n",
    "\n",
    "            # Define state key\n",
    "            player_hand = obs[:11][obs[:11] != 0]\n",
    "            dealer_visible_card = env.dealer[0]\n",
    "            state_key = (tuple(player_hand), dealer_visible_card)\n",
    "\n",
    "            # Record action frequencies\n",
    "            if state_key not in action_frequencies:\n",
    "                action_frequencies[state_key] = {'Hit': 0, 'Stick': 0}\n",
    "            action_frequencies[state_key]['Hit' if action == 1 else 'Stick'] += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            player_hand_sums.append(env.sum_hand(player_hand))\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        player_final_hand = obs[:11][obs[:11] != 0]\n",
    "        dealer_final_hand = obs[11:22][obs[11:22] != 0]\n",
    "\n",
    "        game_results = {\n",
    "            'Game': game + 1,\n",
    "            'PlayerFinalHandSum': env.sum_hand(player_final_hand),\n",
    "            'DealerFinalHandSum': env.sum_hand(dealer_final_hand),\n",
    "            'PlayerNumCards': len(player_final_hand),\n",
    "            'DealerNumCards': len(dealer_final_hand),\n",
    "            'DealerVisibleCard': dealer_visible_card,\n",
    "            'PlayerActions': ' '.join(player_actions),\n",
    "            'PlayerHandProgression': ' '.join(map(str, player_hand_sums)),\n",
    "            'Outcome': 'Win' if reward > 0 else 'Loss' if reward < 0 else 'Draw',\n",
    "            'TotalReward': total_reward  # Add total reward for each game\n",
    "        }\n",
    "        results.append(game_results)\n",
    "\n",
    "    # Export action frequencies and rewards\n",
    "    action_freq_data = []\n",
    "    for state, actions in action_frequencies.items():\n",
    "        player_hand, dealer_card = state\n",
    "        action_freq_data.append({'PlayerHand': ' '.join(map(str, player_hand)), \n",
    "                                 'DealerVisibleCard': dealer_card,\n",
    "                                 'Hit': actions['Hit'], \n",
    "                                 'Stick': actions['Stick'],\n",
    "                                 'TotalActions': actions['Hit'] + actions['Stick']})  # Add total actions for each state\n",
    "\n",
    "    action_freq_df = pd.DataFrame(action_freq_data)\n",
    "    action_freq_df.to_csv('PPO1M_action_frequencies.csv', index=False)\n",
    "    \n",
    "    rewards_df = pd.DataFrame(rewards, columns=['Reward'])\n",
    "    rewards_df.to_csv('PPO1M_rewards_distribution.csv', index=False)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('PPO1M_blackjack_results.csv', index=False)\n",
    "\n",
    "    win_rate = results_df[results_df['Outcome'] == 'Win'].shape[0] / num_games\n",
    "    print(f\"\\nAgent won {results_df[results_df['Outcome'] == 'Win'].shape[0]} out of {num_games} games. Win rate: {win_rate * 100:.2f}%\")\n",
    "    return win_rate\n",
    "\n",
    "model.save(\"PPO1M\")\n",
    "env = SimpleBlackjackEnv()\n",
    "simulate_blackjack_games(env, model)\n",
    "print(f\"Win rate: {win_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a new environment that will allow the agent to also count the card in order to try to increase the winning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(a, b):\n",
    "    if a > b:\n",
    "        return 1\n",
    "    elif a < b:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "class SimpleBlackjackEnvCount(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reward_range = (-np.inf, np.inf)\n",
    "        super(SimpleBlackjackEnv, self).__init__()\n",
    "        self.deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10] * 4  # a full deck\n",
    "        random.shuffle(self.deck)  # shuffle the deck\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.card_count = 0\n",
    "        self.observation_space = spaces.Box(low=np.inf, high=np.inf, shape=(23+1,), dtype=np.float32)  # Added card count to observation\n",
    "\n",
    "    def get_card_value(self, card):\n",
    "        if card in [2, 3, 4, 5, 6]:\n",
    "            return 1\n",
    "        elif card in [7, 8, 9]:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def draw_card(self):\n",
    "        card = self.deck.pop()\n",
    "        self.card_count += self.get_card_value(card)\n",
    "        return card\n",
    "        \n",
    "    def draw_hand(self):\n",
    "        return [self.draw_card(), self.draw_card()]\n",
    "\n",
    "    def usable_ace(self, hand):\n",
    "        return 1 in hand and sum(hand) + 10 <= 21\n",
    "\n",
    "    def sum_hand(self, hand):\n",
    "        if self.usable_ace(hand):\n",
    "            return sum(hand) + 10\n",
    "        return sum(hand)\n",
    "\n",
    "    def is_bust(self, hand):\n",
    "        return self.sum_hand(hand) > 21\n",
    "\n",
    "    def score(self, hand):\n",
    "        return 0 if self.is_bust(hand) else self.sum_hand(hand)\n",
    "\n",
    "    def reset(self):\n",
    "        if len(self.deck) < 15:\n",
    "            self.deck = [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
    "                         10, 10, 10, 10] * 4\n",
    "            random.shuffle(self.deck)\n",
    "            self.card_count = 0  # Reset card count\n",
    "        self.dealer = self.draw_hand()\n",
    "        self.player = self.draw_hand()\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # hit\n",
    "            self.player.append(self.draw_card())\n",
    "            if self.is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -1.0\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0.0\n",
    "        else:  # stick\n",
    "            done = True\n",
    "            while self.sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(self.draw_card())\n",
    "            reward = cmp(self.score(self.player), self.score(self.dealer))\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        player_obs = self.player + [0] * (11 - len(self.player))\n",
    "        dealer_obs = self.dealer + [0] * (11 - len(self.dealer))\n",
    "        usable_ace_obs = [1] if self.usable_ace(self.player) else [0]\n",
    "        card_count_obs = [self.card_count]  # Added card count to observation\n",
    "        return np.array(player_obs + dealer_obs + usable_ace_obs + card_count_obs)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode != 'human':\n",
    "            raise NotImplementedError()\n",
    "        return f\"Player hand: {self.player}, Dealer hand: {self.dealer}, Card count: {self.card_count}\"\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "# Testing the environment to ensure it initializes and steps correctly\n",
    "env = SimpleBlackjackEnv()\n",
    "obs = env.reset()\n",
    "print(env.render())\n",
    "obs, reward, done, _ = env.step(1)\n",
    "print(env.render())\n",
    "obs, reward, done, _ = env.step(0)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(model, env, num_games=1000):\n",
    "    wins = 0\n",
    "    win_rates = []\n",
    "    num_games_list = []  # List to store the number of games after each logging interval\n",
    "\n",
    "    for i in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            if done and reward == 1:\n",
    "                wins += 1\n",
    "        if (i + 1) % 100 == 0:  # Log win rate every 100 games\n",
    "            win_rates.append(wins / (i + 1))\n",
    "            num_games_list.append(i + 1)  # Append the number of games played so far\n",
    "\n",
    "    # Create a DataFrame with both win rates and number of games\n",
    "    win_rate_df = pd.DataFrame({'WinRate': win_rates, 'NumGames': num_games_list})\n",
    "    win_rate_df.to_csv('PPO1M_win_rate_over_time.csv', index=False)\n",
    "    \n",
    "    return wins / num_games\n",
    "env = DummyVecEnv([lambda: SimpleBlackjackEnv()])\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "win_rate = evaluate_agent(agent, env)\n",
    "print(f\"Win rate: {win_rate:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_blackjack_games(env, model, num_games=10000):\n",
    "    action_frequencies = {}\n",
    "    rewards = []\n",
    "    results = []\n",
    "\n",
    "    for game in range(num_games):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        player_actions = []\n",
    "        player_hand_sums = []\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            player_actions.append('Hit' if action == 1 else 'Stick')\n",
    "\n",
    "            # Define state key\n",
    "            player_hand = obs[:11][obs[:11] != 0]\n",
    "            dealer_visible_card = env.dealer[0]\n",
    "            state_key = (tuple(player_hand), dealer_visible_card)\n",
    "\n",
    "            # Record action frequencies\n",
    "            if state_key not in action_frequencies:\n",
    "                action_frequencies[state_key] = {'Hit': 0, 'Stick': 0}\n",
    "            action_frequencies[state_key]['Hit' if action == 1 else 'Stick'] += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            player_hand_sums.append(env.sum_hand(player_hand))\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        player_final_hand = obs[:11][obs[:11] != 0]\n",
    "        dealer_final_hand = obs[11:22][obs[11:22] != 0]\n",
    "\n",
    "        game_results = {\n",
    "            'Game': game + 1,\n",
    "            'PlayerFinalHandSum': env.sum_hand(player_final_hand),\n",
    "            'DealerFinalHandSum': env.sum_hand(dealer_final_hand),\n",
    "            'PlayerNumCards': len(player_final_hand),\n",
    "            'DealerNumCards': len(dealer_final_hand),\n",
    "            'DealerVisibleCard': dealer_visible_card,\n",
    "            'PlayerActions': ' '.join(player_actions),\n",
    "            'PlayerHandProgression': ' '.join(map(str, player_hand_sums)),\n",
    "            'Outcome': 'Win' if reward > 0 else 'Loss' if reward < 0 else 'Draw',\n",
    "            'TotalReward': total_reward  # Add total reward for each game\n",
    "        }\n",
    "        results.append(game_results)\n",
    "\n",
    "    # Export action frequencies and rewards\n",
    "    action_freq_data = []\n",
    "    for state, actions in action_frequencies.items():\n",
    "        player_hand, dealer_card = state\n",
    "        action_freq_data.append({'PlayerHand': ' '.join(map(str, player_hand)), \n",
    "                                 'DealerVisibleCard': dealer_card,\n",
    "                                 'Hit': actions['Hit'], \n",
    "                                 'Stick': actions['Stick'],\n",
    "                                 'TotalActions': actions['Hit'] + actions['Stick']})  # Add total actions for each state\n",
    "\n",
    "    action_freq_df = pd.DataFrame(action_freq_data)\n",
    "    action_freq_df.to_csv('PPO1M_action_frequencies.csv', index=False)\n",
    "    \n",
    "    rewards_df = pd.DataFrame(rewards, columns=['Reward'])\n",
    "    rewards_df.to_csv('PPO1M_rewards_distribution.csv', index=False)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('PPO1M_blackjack_results.csv', index=False)\n",
    "\n",
    "    win_rate = results_df[results_df['Outcome'] == 'Win'].shape[0] / num_games\n",
    "    print(f\"\\nAgent won {results_df[results_df['Outcome'] == 'Win'].shape[0]} out of {num_games} games. Win rate: {win_rate * 100:.2f}%\")\n",
    "    return win_rate\n",
    "\n",
    "\n",
    "simulate_blackjack_games(env, model)\n",
    "print(f\"Win rate: {win_rate:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
